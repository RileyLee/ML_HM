{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "import mnist\n",
    "import scipy\n",
    "import scipy.sparse.linalg\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import identity\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST loaded\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = mnist.load_mnist(\"training\", None, './MNIST');\n",
    "testX, testY = mnist.load_mnist(\"testing\", None, './MNIST');\n",
    "print(\"MNIST loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class logistic:\n",
    "    \n",
    "    \n",
    "    def __init__(self, eta, lamda, thresh, batchSize):\n",
    "        self.eta = eta;\n",
    "        self.lamda = lamda;\n",
    "        self.thresh = thresh;\n",
    "        self.regularized = True;\n",
    "        self.lossSet = np.zeros(100000);\n",
    "        self.testLossSet = np.zeros(100000);\n",
    "        self.loss01Set = np.zeros(100000);\n",
    "        self.testLoss01Set = np.zeros(100000);\n",
    "        self.trainLossSet = np.zeros(100000);\n",
    "        self.trainLoss01Set = np.zeros(100000);\n",
    "        self.batchSize = batchSize;\n",
    "        self.stoch = True;\n",
    "        \n",
    "    def load_train(self, trainX, trainY):\n",
    "        self.trainY1 = trainY;\n",
    "        s = trainX.shape;\n",
    "        self.d = s[1] * s[2] + 1;\n",
    "        self.n = self.d;\n",
    "        self.N = s[0];\n",
    "        self.trainX = np.reshape(trainX, (self.N, self.d-1));\n",
    "        self.trainX = np.asmatrix(np.insert(self.trainX, 0, 1, axis=1));\n",
    "        self.trainY = np.zeros((self.N, 10),dtype=np.float)\n",
    "  \n",
    "        for i in range(0,10):\n",
    "            temp = np.reshape(np.array(trainY == i, dtype=float), (self.N, 1));\n",
    "            self.trainY[:,i] = np.asmatrix(np.reshape(temp, self.N))\n",
    "            \n",
    "        if self.stoch==False:\n",
    "            self.probTr = np.asmatrix(np.zeros((self.N, 10),dtype=float))\n",
    "        else:\n",
    "            self.probTr = np.asmatrix(np.zeros((self.batchSize, 10),dtype=float))\n",
    "        self.probTr1 = np.asmatrix(np.zeros((self.N, 10),dtype=float))\n",
    "        print(\"Training data loaded...\")\n",
    "        \n",
    "    def load_test(self, testX, testY):\n",
    "        self.testY1 = testY;\n",
    "        self.testN = testX.shape[0];\n",
    "        self.testX = np.reshape(testX, (self.testN, self.d-1));\n",
    "        self.testX = np.asmatrix(np.insert(self.testX, 0, 1, axis=1));\n",
    "        self.testY = np.zeros((self.testN, 10),dtype=np.float)\n",
    "        self.probTe = np.asmatrix(np.zeros((self.testN, 10),dtype=float))\n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        for i in range(0,10):\n",
    "            temp = np.reshape(np.array(testY == i, dtype=float), (self.testN, 1));\n",
    "            self.testY[:,i] = np.asmatrix(np.reshape(temp, self.testN))\n",
    "        print(\"Testing data loaded...\")\n",
    "        \n",
    "    def load_batch(self):\n",
    "        self.Y1 = self.trainY1[self.batchFrom:self.batchTo]\n",
    "        self.X = self.trainX[self.batchFrom:self.batchTo, :]\n",
    "        self.Y = self.trainY[self.batchFrom:self.batchTo, :]\n",
    "        self.batchFrom += self.batchSize\n",
    "        self.batchTo += self.batchSize\n",
    "        self.batchFrom = (self.batchFrom - self.N) if self.batchFrom >= self.N else self.batchFrom\n",
    "        self.batchTo = (self.batchTo - self.N) if self.batchTo > self.N else self.batchTo\n",
    "    \n",
    "    def initIteration(self):\n",
    "        self.weights = np.asmatrix(np.zeros((self.n,10), dtype=float));\n",
    "        self.iter = 0;\n",
    "        self.prevLoss = 999999;\n",
    "        self.weightDist = 99999;\n",
    "        self.batchFrom = 0;\n",
    "        self.batchTo = self.batchSize;\n",
    "        \n",
    "    def computeProb(self):\n",
    "        \n",
    "        temp = self.X * self.weights;\n",
    "        temp[temp>100] = 100\n",
    "        \n",
    "        temp = np.exp(temp);\n",
    "        sumCol = np.sum(temp, axis=1)\n",
    "        \n",
    "        for i in range(0,10):\n",
    "            self.probTr[:,i] = np.divide(temp[:,i], sumCol) \n",
    "        self.sampleLoss = self.Y - self.probTr; \n",
    "        \n",
    "    def computeLoss(self):\n",
    "        self.loss = -np.sum(np.log(np.sum(np.multiply(self.Y, self.probTr), axis=1) + 0.0000001)) / self.batchSize; \n",
    "        self.lossSet[self.iter]=self.loss\n",
    "        \n",
    "        self.predLabel = np.argmax(self.probTr, axis=1);\n",
    "        temp = (np.array(self.predLabel) != np.reshape(self.Y1, (self.batchSize, 1))).astype(float)\n",
    "        self.loss01Set[self.iter] = np.sum(temp) / self.batchSize;\n",
    "    \n",
    "    def computeTestLoss(self):\n",
    "        temp = self.testX * self.weights;\n",
    "        temp[temp>100] = 100\n",
    "        \n",
    "        temp = np.exp(temp);\n",
    "        sumCol = np.sum(temp, axis=1)\n",
    "        \n",
    "        for i in range(0,10):\n",
    "            self.probTe[:,i] = np.divide(temp[:,i], sumCol) \n",
    "        \n",
    "        self.testLoss = -np.sum(np.log(np.sum(np.multiply(self.testY, self.probTe), axis=1) + 0.0000001)) / self.testN; \n",
    "        self.testLossSet[self.iter]=self.testLoss\n",
    "        \n",
    "        self.predLabelTe = np.argmax(self.probTe, axis=1);\n",
    "        temp = (np.array(self.predLabelTe) != np.reshape(self.testY1, (self.testN, 1))).astype(float)\n",
    "        self.testLoss01Set[self.iter] = np.sum(temp) / self.testN;\n",
    "        \n",
    "    def computeTrainLoss(self):\n",
    "        temp = self.trainX * self.weights;\n",
    "        temp[temp>100] = 100\n",
    "        \n",
    "        temp = np.exp(temp);\n",
    "        sumCol = np.sum(temp, axis=1)\n",
    "        \n",
    "        for i in range(0,10):\n",
    "            self.probTr1[:,i] = np.divide(temp[:,i], sumCol) \n",
    "        \n",
    "        self.trainLoss = -np.sum(np.log(np.sum(np.multiply(self.trainY, self.probTr1), axis=1) + 0.0000001)) / self.N; \n",
    "        \n",
    "    def updateWeightsWhole(self):\n",
    "        deriv = self.Y - self.probTr;\n",
    "        if self.stoch==True:\n",
    "            self.weights = self.weights + self.eta * self.X.transpose() * deriv / self.batchSize\n",
    "            self.weights = self.weights - self.eta * self.weights * self.lamda;\n",
    "        else:\n",
    "            self.weights = self.weights + self.eta * self.X.transpose() * deriv / self.N\n",
    "    \n",
    "    def distWeight(self, A, B):\n",
    "        self.weightDist = scipy.linalg.norm(A - B);\n",
    "        \n",
    "    \n",
    "    def assess(self):\n",
    "        self.computeProb1();\n",
    "        self.pred = self.prob1 > 0.5;\n",
    "        self.correct = np.sum(np.int16(self.pred == (self.Y==1)))\n",
    "        self.overallAccu = np.float(self.correct) / np.float(self.N);\n",
    "        self.loss01 = sum(abs(self.pred - self.Y)) / self.N\n",
    "        print('Accuracy : ' + str(self.overallAccu))\n",
    "        print('0 / 1 loss : ' + str(self.loss01))\n",
    "    \n",
    "    def genGaussFeatWeight(self, n):\n",
    "        self.n = n;\n",
    "        self.weight = np.asmatrix(np.random.randn(self.d, self.n));\n",
    "    \n",
    "    def randGaussFeatConv(self):\n",
    "        trainX = self.trainX * self.weight;\n",
    "        testX = self.testX * self.weight;\n",
    "        \n",
    "        self.trainX = np.reshape(trainX, (self.N, self.n));\n",
    "        self.trainX = np.asmatrix(np.insert(self.trainX, 0, 1, axis=1));\n",
    "        self.testX = np.reshape(testX, (self.testN, self.n));\n",
    "        self.testX = np.asmatrix(np.insert(self.testX, 0, 1, axis=1));\n",
    "        \n",
    "        self.d = self.n;\n",
    "        self.n = self.n+1;\n",
    "        print(\"Feature generation complete....\")\n",
    "        \n",
    "    def plotLoss(self, labelTrain, labelTest, name):\n",
    "        red_star = matplotlib.pyplot.plot(np.arange(0, self.iter), model.lossSet[0:self.iter], color=\"red\", hold = True, label=labelTrain, linewidth=2.0)\n",
    "        blue_star = matplotlib.pyplot.plot(np.arange(0, self.iter), model.testLossSet[0:self.iter], hold = True, color=\"blue\", label=labelTest, linewidth=2.0)\n",
    "        matplotlib.pyplot.savefig(name, transparent = True)\n",
    "    \n",
    "    def plotLoss01(self, labelTrain, labelTest, name):\n",
    "        red_star = matplotlib.pyplot.plot(np.arange(0, self.iter), model.loss01Set[0:self.iter], color=\"red\", hold = True, label=labelTrain, linewidth=2.0)\n",
    "        blue_star = matplotlib.pyplot.plot(np.arange(0, self.iter), model.testLoss01Set[0:self.iter], hold = True, color=\"blue\", label=labelTest, linewidth=2.0)\n",
    "        matplotlib.pyplot.savefig(name, transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded...\n",
      "Testing data loaded...\n",
      "Feature generation complete....\n"
     ]
    }
   ],
   "source": [
    "model = logistic(0.00001, 0.00001, 0.001, 10000);\n",
    "\n",
    "model.load_train(trainX, trainY);\n",
    "model.load_test(testX, testY);\n",
    "\n",
    "model.genGaussFeatWeight(100);\n",
    "model.randGaussFeatConv();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : Loss value 2.30258409299\n",
      "Iteration 100 : Average Loss value 2.22201592146\n",
      "weight distance : 0.000102865758673\n",
      "Testing 0/1 loss : 0.383\n",
      "Iteration 200 : Average Loss value 2.1202476741\n",
      "weight distance : 0.000104562778341\n",
      "Testing 0/1 loss : 0.3582\n",
      "Iteration 300 : Average Loss value 2.02428670466\n",
      "weight distance : 0.000105110405778\n",
      "Testing 0/1 loss : 0.34\n",
      "Iteration 400 : Average Loss value 1.9342188602\n",
      "weight distance : 0.000101700942072\n",
      "Testing 0/1 loss : 0.3231\n",
      "Iteration 500 : Average Loss value 1.84851393025\n",
      "weight distance : 0.000104404394546\n",
      "Testing 0/1 loss : 0.3128\n",
      "Iteration 600 : Average Loss value 1.76776054126\n",
      "weight distance : 0.000104859595915\n",
      "Testing 0/1 loss : 0.3013\n",
      "Iteration 700 : Average Loss value 1.69274768336\n",
      "weight distance : 0.000101646236531\n",
      "Testing 0/1 loss : 0.2915\n",
      "Iteration 800 : Average Loss value 1.62120356383\n",
      "weight distance : 0.000104607550382\n",
      "Testing 0/1 loss : 0.2815\n",
      "Iteration 900 : Average Loss value 1.5541536201\n",
      "weight distance : 0.000104906618001\n",
      "Testing 0/1 loss : 0.2734\n",
      "Iteration 1000 : Average Loss value 1.49271414613\n",
      "weight distance : 0.000101716786672\n",
      "Testing 0/1 loss : 0.2676\n",
      "Iteration 1100 : Average Loss value 1.43389463781\n",
      "weight distance : 0.000104778547035\n",
      "Testing 0/1 loss : 0.2615\n",
      "Iteration 1200 : Average Loss value 1.37896049655\n",
      "weight distance : 0.000104969660922\n",
      "Testing 0/1 loss : 0.2559\n",
      "Iteration 1300 : Average Loss value 1.32938800699\n",
      "weight distance : 0.000101776033366\n",
      "Testing 0/1 loss : 0.2523\n",
      "Iteration 1400 : Average Loss value 1.28161095301\n",
      "weight distance : 0.000104905943721\n",
      "Testing 0/1 loss : 0.2492\n",
      "Iteration 1500 : Average Loss value 1.23703813208\n",
      "weight distance : 0.000105027876811\n",
      "Testing 0/1 loss : 0.2451\n",
      "Iteration 1600 : Average Loss value 1.19753500973\n",
      "weight distance : 0.000101819755697\n",
      "Testing 0/1 loss : 0.243\n",
      "Iteration 1700 : Average Loss value 1.15907480251\n",
      "weight distance : 0.00010500669932\n",
      "Testing 0/1 loss : 0.24\n",
      "Iteration 1800 : Average Loss value 1.12314373236\n",
      "weight distance : 0.000105088751\n",
      "Testing 0/1 loss : 0.2375\n",
      "Iteration 1900 : Average Loss value 1.09200255643\n",
      "weight distance : 0.000101856205581\n",
      "Testing 0/1 loss : 0.2357\n",
      "Iteration 2000 : Average Loss value 1.061249131\n",
      "weight distance : 0.00010508815139\n",
      "Testing 0/1 loss : 0.2345\n",
      "Iteration 2100 : Average Loss value 1.03240377641\n",
      "weight distance : 0.000105152907831\n",
      "Testing 0/1 loss : 0.2343\n",
      "Iteration 2200 : Average Loss value 1.00811127567\n",
      "weight distance : 0.000101890497934\n",
      "Testing 0/1 loss : 0.2332\n",
      "Iteration 2300 : Average Loss value 0.983653714719\n",
      "weight distance : 0.000105153546269\n",
      "Testing 0/1 loss : 0.2323\n",
      "Iteration 2400 : Average Loss value 0.960559325083\n",
      "weight distance : 0.000105217231827\n",
      "Testing 0/1 loss : 0.2306\n",
      "Iteration 2500 : Average Loss value 0.941835577159\n",
      "weight distance : 0.000101924885647\n",
      "Testing 0/1 loss : 0.2301\n",
      "Iteration 2600 : Average Loss value 0.92248834386\n",
      "weight distance : 0.000105205600449\n",
      "Testing 0/1 loss : 0.2299\n",
      "Iteration 2700 : Average Loss value 0.904039699519\n",
      "weight distance : 0.000105278790594\n",
      "Testing 0/1 loss : 0.2291\n",
      "Iteration 2800 : Average Loss value 0.889834809512\n",
      "weight distance : 0.000101959844477\n",
      "Testing 0/1 loss : 0.228\n",
      "Iteration 2900 : Average Loss value 0.874628404031\n",
      "weight distance : 0.000105246940942\n",
      "Testing 0/1 loss : 0.2263\n",
      "Iteration 3000 : Average Loss value 0.85993033138\n",
      "weight distance : 0.000105335920725\n",
      "Testing 0/1 loss : 0.226\n",
      "Iteration 3100 : Average Loss value 0.849399113885\n",
      "weight distance : 0.00010199489944\n",
      "Testing 0/1 loss : 0.2257\n",
      "Iteration 3200 : Average Loss value 0.837553489075\n",
      "weight distance : 0.000105279866095\n",
      "Testing 0/1 loss : 0.2256\n",
      "Iteration 3300 : Average Loss value 0.825889952562\n",
      "weight distance : 0.000105388024248\n",
      "Testing 0/1 loss : 0.2257\n",
      "Iteration 3400 : Average Loss value 0.818358906099\n",
      "weight distance : 0.000102029233618\n",
      "Testing 0/1 loss : 0.2249\n",
      "Iteration 3500 : Average Loss value 0.809251697505\n",
      "weight distance : 0.000105306226511\n",
      "Testing 0/1 loss : 0.2248\n",
      "Iteration 3600 : Average Loss value 0.800053190598\n",
      "weight distance : 0.000105435127639\n",
      "Testing 0/1 loss : 0.2247\n",
      "Iteration 3700 : Average Loss value 0.794987766962\n",
      "weight distance : 0.00010206205796\n",
      "Testing 0/1 loss : 0.2244\n",
      "Iteration 3800 : Average Loss value 0.788123785391\n",
      "weight distance : 0.000105327452386\n",
      "Testing 0/1 loss : 0.2248\n",
      "Iteration 3900 : Average Loss value 0.780937708702\n",
      "weight distance : 0.000105477548314\n",
      "Testing 0/1 loss : 0.2246\n",
      "Iteration 4000 : Average Loss value 0.777913431553\n",
      "weight distance : 0.000102092777222\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 4100 : Average Loss value 0.775862129973\n",
      "weight distance : 1.05338681903e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 4200 : Average Loss value 0.773858091356\n",
      "weight distance : 1.0549349885e-05\n",
      "Testing 0/1 loss : 0.224\n",
      "Iteration 4300 : Average Loss value 0.775482205631\n",
      "weight distance : 1.02095350921e-05\n",
      "Testing 0/1 loss : 0.224\n",
      "Iteration 4400 : Average Loss value 0.774512452815\n",
      "weight distance : 1.05340337239e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 4500 : Average Loss value 0.772517225511\n",
      "weight distance : 1.05497325786e-05\n",
      "Testing 0/1 loss : 0.2237\n",
      "Iteration 4600 : Average Loss value 0.774165491605\n",
      "weight distance : 1.02098273756e-05\n",
      "Testing 0/1 loss : 0.2237\n",
      "Iteration 4700 : Average Loss value 0.773212622913\n",
      "weight distance : 1.05341957535e-05\n",
      "Testing 0/1 loss : 0.2238\n",
      "Iteration 4800 : Average Loss value 0.771225925788\n",
      "weight distance : 1.05501112484e-05\n",
      "Testing 0/1 loss : 0.2238\n",
      "Iteration 4900 : Average Loss value 0.772898033\n",
      "weight distance : 1.02101170654e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 5000 : Average Loss value 0.771961722968\n",
      "weight distance : 1.05343543655e-05\n",
      "Testing 0/1 loss : 0.224\n",
      "Iteration 5100 : Average Loss value 0.769983281073\n",
      "weight distance : 1.05504859364e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 5200 : Average Loss value 0.771678925672\n",
      "weight distance : 1.02104041463e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 5300 : Average Loss value 0.770758855542\n",
      "weight distance : 1.05345096435e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 5400 : Average Loss value 0.768788399967\n",
      "weight distance : 1.05508566845e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 5500 : Average Loss value 0.770507285005\n",
      "weight distance : 1.02106886043e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 5600 : Average Loss value 0.769603142558\n",
      "weight distance : 1.05346616683e-05\n",
      "Testing 0/1 loss : 0.2238\n",
      "Iteration 5700 : Average Loss value 0.767640410299\n",
      "weight distance : 1.05512235343e-05\n",
      "Testing 0/1 loss : 0.2237\n",
      "Iteration 5800 : Average Loss value 0.769382245431\n",
      "weight distance : 1.02109704272e-05\n",
      "Testing 0/1 loss : 0.2237\n",
      "Iteration 5900 : Average Loss value 0.768493724826\n",
      "weight distance : 1.05348105182e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 6000 : Average Loss value 0.766538458654\n",
      "weight distance : 1.0551586527e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 6100 : Average Loss value 0.768302959963\n",
      "weight distance : 1.0211249604e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 6200 : Average Loss value 0.767429761575\n",
      "weight distance : 1.05349562688e-05\n",
      "Testing 0/1 loss : 0.224\n",
      "Iteration 6300 : Average Loss value 0.765481709904\n",
      "weight distance : 1.05519457038e-05\n",
      "Testing 0/1 loss : 0.224\n",
      "Iteration 6400 : Average Loss value 0.767268599733\n",
      "weight distance : 1.02115261253e-05\n",
      "Testing 0/1 loss : 0.224\n",
      "Iteration 6500 : Average Loss value 0.766410429999\n",
      "weight distance : 1.05350989935e-05\n",
      "Testing 0/1 loss : 0.224\n",
      "Iteration 6600 : Average Loss value 0.76446934676\n",
      "weight distance : 1.05523011056e-05\n",
      "Testing 0/1 loss : 0.2238\n",
      "Iteration 6700 : Average Loss value 0.766278353556\n",
      "weight distance : 1.02117999832e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 6800 : Average Loss value 0.765434924821\n",
      "weight distance : 1.05352387634e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 6900 : Average Loss value 0.763500569335\n",
      "weight distance : 1.05526527727e-05\n",
      "Testing 0/1 loss : 0.2239\n",
      "Iteration 7000 : Average Loss value 0.765331427492\n",
      "weight distance : 1.0212071171e-05\n",
      "Testing 0/1 loss : 0.2237\n",
      "Iteration 7100 : Average Loss value 0.764502457864\n",
      "weight distance : 1.05353756472e-05\n",
      "Testing 0/1 loss : 0.2237\n",
      "Iteration 7200 : Average Loss value 0.762574594721\n",
      "weight distance : 1.05530007454e-05\n",
      "Testing 0/1 loss : 0.2237\n",
      "Iteration 7300 : Average Loss value 0.764427044439\n",
      "weight distance : 1.02123396835e-05\n",
      "Testing 0/1 loss : 0.2238\n",
      "Iteration 7400 : Average Loss value 0.763612257639\n",
      "weight distance : 1.05355097117e-05\n",
      "Testing 0/1 loss : 0.2237\n",
      "Iteration 7500 : Average Loss value 0.761690656575\n",
      "weight distance : 1.05533450634e-05\n",
      "Testing 0/1 loss : 0.2236\n",
      "Iteration 7600 : Average Loss value 0.76356444372\n",
      "weight distance : 1.02126055167e-05\n",
      "Testing 0/1 loss : 0.2236\n",
      "Iteration 7700 : Average Loss value 0.762763568947\n",
      "weight distance : 1.05356410218e-05\n",
      "Testing 0/1 loss : 0.2235\n",
      "Iteration 7800 : Average Loss value 0.760848004723\n",
      "weight distance : 1.05536857663e-05\n",
      "Testing 0/1 loss : 0.2235\n",
      "Iteration 7900 : Average Loss value 0.762742880697\n",
      "weight distance : 1.02128686681e-05\n",
      "Testing 0/1 loss : 0.2235\n",
      "Iteration 8000 : Average Loss value 0.761955652484\n",
      "weight distance : 1.05357696402e-05\n",
      "Testing 0/1 loss : 0.2235\n",
      "Iteration 8100 : Average Loss value 0.760045904767\n",
      "weight distance : 1.05540228931e-05\n",
      "Testing 0/1 loss : 0.2233\n",
      "Iteration 8200 : Average Loss value 0.761961626386\n",
      "weight distance : 1.02131291362e-05\n",
      "Testing 0/1 loss : 0.2233\n",
      "Iteration 8300 : Average Loss value 0.761187784469\n",
      "weight distance : 1.05358956279e-05\n",
      "Testing 0/1 loss : 0.2233\n",
      "Iteration 8400 : Average Loss value 0.759283637709\n",
      "weight distance : 1.05543564825e-05\n",
      "Testing 0/1 loss : 0.2234\n",
      "Iteration 8500 : Average Loss value 0.761219967091\n",
      "weight distance : 1.0213386921e-05\n",
      "Testing 0/1 loss : 0.2234\n",
      "Iteration 8600 : Average Loss value 0.760459256273\n",
      "weight distance : 1.05360190441e-05\n",
      "Testing 0/1 loss : 0.2234\n",
      "Iteration 8700 : Average Loss value 0.758560499584\n",
      "weight distance : 1.0554686573e-05\n",
      "Testing 0/1 loss : 0.2232\n",
      "Iteration 8800 : Average Loss value 0.760517204036\n",
      "weight distance : 1.02136420234e-05\n",
      "Testing 0/1 loss : 0.2231\n",
      "Iteration 8900 : Average Loss value 0.759769374062\n",
      "weight distance : 1.05361399463e-05\n",
      "Testing 0/1 loss : 0.2231\n",
      "Iteration 9000 : Average Loss value 0.757875801103\n",
      "weight distance : 1.05550132023e-05\n",
      "Testing 0/1 loss : 0.2232\n",
      "Iteration 9100 : Average Loss value 0.759852653022\n",
      "weight distance : 1.02138944457e-05\n",
      "Testing 0/1 loss : 0.2232\n",
      "Iteration 9200 : Average Loss value 0.759117458446\n",
      "weight distance : 1.05362583903e-05\n",
      "Testing 0/1 loss : 0.2231\n",
      "Iteration 9300 : Average Loss value 0.757228867301\n",
      "weight distance : 1.0555336408e-05\n",
      "Testing 0/1 loss : 0.2231\n",
      "Iteration 9400 : Average Loss value 0.759225644074\n",
      "weight distance : 1.02141441911e-05\n",
      "Testing 0/1 loss : 0.2231\n",
      "Iteration 9500 : Average Loss value 0.75850284414\n",
      "weight distance : 1.05363744304e-05\n",
      "Testing 0/1 loss : 0.2232\n",
      "Iteration 9600 : Average Loss value 0.756619037192\n",
      "weight distance : 1.05556562273e-05\n",
      "Testing 0/1 loss : 0.2232\n",
      "Iteration 9700 : Average Loss value 0.758635521108\n",
      "weight distance : 1.02143912639e-05\n",
      "Testing 0/1 loss : 0.2232\n",
      "Iteration 9800 : Average Loss value 0.757924879626\n",
      "weight distance : 1.05364881193e-05\n",
      "Testing 0/1 loss : 0.2232\n",
      "Iteration 9900 : Average Loss value 0.756045663442\n",
      "weight distance : 1.05559726968e-05\n",
      "Testing 0/1 loss : 0.2232\n",
      "Iteration 10000 : Average Loss value 0.758081641601\n",
      "weight distance : 1.02146356696e-05\n",
      "Testing 0/1 loss : 0.2234\n",
      "Iteration 10100 : Average Loss value 0.757382926823\n",
      "weight distance : 1.05365995083e-05\n",
      "Testing 0/1 loss : 0.2234\n",
      "Iteration 10200 : Average Loss value 0.755508112028\n",
      "weight distance : 1.05562858529e-05\n",
      "Testing 0/1 loss : 0.2234\n",
      "Iteration 10300 : Average Loss value 0.757563376259\n",
      "weight distance : 1.02148774144e-05\n",
      "Testing 0/1 loss : 0.2233\n",
      "Iteration 10400 : Average Loss value 0.756876360766\n",
      "weight distance : 1.05367086473e-05\n",
      "Testing 0/1 loss : 0.2233\n",
      "Iteration 10500 : Average Loss value 0.755005761924\n",
      "weight distance : 1.05565957314e-05\n",
      "Testing 0/1 loss : 0.2232\n",
      "Iteration 10600 : Average Loss value 0.757080108701\n",
      "weight distance : 1.02151165057e-05\n",
      "Testing 0/1 loss : 0.2233\n",
      "Iteration 10700 : Average Loss value 0.756404569284\n",
      "weight distance : 1.05368155847e-05\n",
      "Testing 0/1 loss : 0.2233\n",
      "Iteration 10800 : Average Loss value 0.754538004778\n",
      "weight distance : 1.05569023679e-05\n",
      "Testing 0/1 loss : 0.2233\n",
      "Iteration 10900 : Average Loss value 0.756631235141\n",
      "weight distance : 1.02153529517e-05\n",
      "Testing 0/1 loss : 0.2232\n",
      "Iteration 11000 : Average Loss value 0.755966952686\n",
      "weight distance : 1.05369203677e-05\n",
      "Testing 0/1 loss : 0.2234\n",
      "Iteration 11100 : Average Loss value 0.7541042446\n",
      "weight distance : 1.05572057973e-05\n",
      "Testing 0/1 loss : 0.2234\n",
      "Iteration 11200 : Average Loss value 0.756216164074\n",
      "weight distance : 1.02155867615e-05\n",
      "Testing 0/1 loss : 0.2233\n",
      "Iteration 11300 : Average Loss value 0.755562923455\n",
      "weight distance : 1.05370230423e-05\n",
      "Testing 0/1 loss : 0.2233\n",
      "Iteration 11400 : Average Loss value 0.753703897456\n",
      "weight distance : 1.05575060543e-05\n",
      "Testing 0/1 loss : 0.2234\n",
      "Iteration 11500 : Average Loss value 0.75583431597\n",
      "weight distance : 1.02158179449e-05\n",
      "Testing 0/1 loss : 0.2235\n",
      "Iteration 11600 : Average Loss value 0.755191905937\n",
      "weight distance : 1.05371236531e-05\n",
      "Testing 0/1 loss : 0.2235\n",
      "Iteration 11700 : Average Loss value 0.753336391168\n",
      "weight distance : 1.05578031731e-05\n",
      "Testing 0/1 loss : 0.2234\n",
      "Iteration 11800 : Average Loss value 0.755485122975\n",
      "weight distance : 1.02160465127e-05\n",
      "Testing 0/1 loss : 0.2234\n",
      "Iteration 11900 : Average Loss value 0.754853336053\n",
      "weight distance : 1.05372222436e-05\n",
      "Testing 0/1 loss : 0.2233\n",
      "Iteration 12000 : Average Loss value 0.753001165015\n",
      "weight distance : 1.05580971875e-05\n",
      "Testing 0/1 loss : 0.2232\n",
      "Iteration 12100 : Average Loss value 0.755168028609\n",
      "weight distance : 1.02162724764e-05\n",
      "Testing 0/1 loss : 0.2231\n",
      "Iteration 12200 : Average Loss value 0.754546660996\n",
      "weight distance : 1.05373188563e-05\n",
      "Testing 0/1 loss : 0.223\n",
      "Iteration 12300 : Average Loss value 0.752697669451\n",
      "weight distance : 1.05583881309e-05\n",
      "Testing 0/1 loss : 0.223\n",
      "Iteration 12400 : Average Loss value 0.754882487487\n",
      "weight distance : 1.02164958481e-05\n",
      "Testing 0/1 loss : 0.223\n",
      "Iteration 12500 : Average Loss value 0.754271338956\n",
      "weight distance : 1.05374135324e-05\n",
      "Testing 0/1 loss : 0.223\n",
      "Iteration 12600 : Average Loss value 0.752425365826\n",
      "weight distance : 1.05586760362e-05\n",
      "Testing 0/1 loss : 0.223\n",
      "Iteration 12700 : Average Loss value 0.754627965027\n",
      "weight distance : 1.02167166407e-05\n",
      "Testing 0/1 loss : 0.2229\n",
      "Iteration 12800 : Average Loss value 0.754026838842\n",
      "weight distance : 1.0537506312e-05\n",
      "Testing 0/1 loss : 0.2228\n",
      "Iteration 12900 : Average Loss value 0.752183726113\n",
      "weight distance : 1.05589609361e-05\n",
      "Testing 0/1 loss : 0.2228\n",
      "Iteration 13000 : Average Loss value 0.754403937188\n",
      "weight distance : 1.02169348678e-05\n",
      "Testing 0/1 loss : 0.2228\n",
      "Iteration 13100 : Average Loss value 0.753812640013\n",
      "weight distance : 1.05375972345e-05\n",
      "Testing 0/1 loss : 0.2228\n",
      "Iteration 13200 : Average Loss value 0.751972232651\n",
      "weight distance : 1.05592428628e-05\n",
      "Testing 0/1 loss : 0.2228\n",
      "Iteration 13300 : Average Loss value 0.7542098902\n",
      "weight distance : 1.02171505435e-05\n",
      "Testing 0/1 loss : 0.2227\n",
      "Iteration 13400 : Average Loss value 0.753628232023\n",
      "weight distance : 1.05376863379e-05\n",
      "Testing 0/1 loss : 0.2227\n",
      "Iteration 13500 : Average Loss value 0.751790377892\n",
      "weight distance : 1.05595218478e-05\n",
      "Testing 0/1 loss : 0.2227\n",
      "Iteration 13600 : Average Loss value 0.754045320319\n",
      "weight distance : 1.02173636825e-05\n",
      "Testing 0/1 loss : 0.2228\n",
      "Iteration 13700 : Average Loss value 0.75347311438\n",
      "weight distance : 1.05377736594e-05\n",
      "Testing 0/1 loss : 0.2227\n",
      "Iteration 13800 : Average Loss value 0.751637664167\n",
      "weight distance : 1.05597979227e-05\n",
      "Testing 0/1 loss : 0.2227\n",
      "Iteration 13900 : Average Loss value 0.75390973358\n",
      "weight distance : 1.02175743002e-05\n",
      "Testing 0/1 loss : 0.2226\n",
      "Iteration 14000 : Average Loss value 0.753346796302\n",
      "weight distance : 1.05378592354e-05\n",
      "Testing 0/1 loss : 0.2225\n",
      "Iteration 14100 : Average Loss value 0.751513603455\n",
      "weight distance : 1.05600711184e-05\n",
      "Testing 0/1 loss : 0.2225\n",
      "Iteration 14200 : Average Loss value 0.753802645567\n",
      "weight distance : 1.02177824124e-05\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 14300 : Average Loss value 0.753248796503\n",
      "weight distance : 1.05379431013e-05\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 14400 : Average Loss value 0.751417717164\n",
      "weight distance : 1.05603414654e-05\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 14500 : Average Loss value 0.7537235812\n",
      "weight distance : 1.02179880353e-05\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 14600 : Average Loss value 0.753178642974\n",
      "weight distance : 1.05380252916e-05\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 14700 : Average Loss value 0.751349535932\n",
      "weight distance : 1.05606089939e-05\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 14800 : Average Loss value 0.753672074521\n",
      "weight distance : 1.02181911858e-05\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 14900 : Average Loss value 0.753135872787\n",
      "weight distance : 1.05381058398e-05\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 15000 : Average Loss value 0.751308599425\n",
      "weight distance : 1.05608737338e-05\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 15100 : Average Loss value 0.753651132912\n",
      "weight distance : 1.02183294442e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 15200 : Average Loss value 0.75312559025\n",
      "weight distance : 1.0538136665e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 15300 : Average Loss value 0.751304695764\n",
      "weight distance : 1.05609013043e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 15400 : Average Loss value 0.753649564633\n",
      "weight distance : 1.02183494543e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 15500 : Average Loss value 0.753124074197\n",
      "weight distance : 1.05381445421e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 15600 : Average Loss value 0.751302550414\n",
      "weight distance : 1.05609275886e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 15700 : Average Loss value 0.753648263429\n",
      "weight distance : 1.021836944e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 15800 : Average Loss value 0.753122824862\n",
      "weight distance : 1.05381524034e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 15900 : Average Loss value 0.751300671766\n",
      "weight distance : 1.05609538453e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 16000 : Average Loss value 0.753647228858\n",
      "weight distance : 1.02183894013e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 16100 : Average Loss value 0.753121841802\n",
      "weight distance : 1.05381602489e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 16200 : Average Loss value 0.751299059377\n",
      "weight distance : 1.05609800747e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 16300 : Average Loss value 0.753646460473\n",
      "weight distance : 1.02184093381e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 16400 : Average Loss value 0.753121124572\n",
      "weight distance : 1.05381680787e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 16500 : Average Loss value 0.751297712802\n",
      "weight distance : 1.05610062767e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 16600 : Average Loss value 0.753645957833\n",
      "weight distance : 1.02184292507e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 16700 : Average Loss value 0.75312067273\n",
      "weight distance : 1.05381758929e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 16800 : Average Loss value 0.7512966316\n",
      "weight distance : 1.05610324513e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 16900 : Average Loss value 0.753645720495\n",
      "weight distance : 1.02184491388e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 17000 : Average Loss value 0.753120485835\n",
      "weight distance : 1.05381836913e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 17100 : Average Loss value 0.751295815327\n",
      "weight distance : 1.05610585985e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 17200 : Average Loss value 0.753645748019\n",
      "weight distance : 1.02184690026e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 17300 : Average Loss value 0.753120563446\n",
      "weight distance : 1.05381914742e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 17400 : Average Loss value 0.751295263544\n",
      "weight distance : 1.05610847184e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 17500 : Average Loss value 0.753646039964\n",
      "weight distance : 1.02184888421e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 17600 : Average Loss value 0.753120905123\n",
      "weight distance : 1.05381992415e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 17700 : Average Loss value 0.751294975811\n",
      "weight distance : 1.0561110811e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 17800 : Average Loss value 0.75364659589\n",
      "weight distance : 1.02185086572e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 17900 : Average Loss value 0.753121510426\n",
      "weight distance : 1.05382069933e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 18000 : Average Loss value 0.751294951688\n",
      "weight distance : 1.05611368764e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 18100 : Average Loss value 0.753647415358\n",
      "weight distance : 1.02185284481e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 18200 : Average Loss value 0.753122378917\n",
      "weight distance : 1.05382147295e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 18300 : Average Loss value 0.751295190737\n",
      "weight distance : 1.05611629145e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 18400 : Average Loss value 0.753648497931\n",
      "weight distance : 1.02185482147e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 18500 : Average Loss value 0.75312351016\n",
      "weight distance : 1.05382224502e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 18600 : Average Loss value 0.75129569252\n",
      "weight distance : 1.05611889255e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 18700 : Average Loss value 0.753649843173\n",
      "weight distance : 1.0218567957e-06\n",
      "Testing 0/1 loss : 0.2224\n",
      "Iteration 18800 : Average Loss value 0.753124903716\n",
      "weight distance : 1.05382301555e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 18900 : Average Loss value 0.751296456601\n",
      "weight distance : 1.05612149092e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 19000 : Average Loss value 0.753651450646\n",
      "weight distance : 1.02185876752e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 19100 : Average Loss value 0.753126559152\n",
      "weight distance : 1.05382378454e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 19200 : Average Loss value 0.751297482545\n",
      "weight distance : 1.05612408658e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 19300 : Average Loss value 0.753653319915\n",
      "weight distance : 1.02186073691e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 19400 : Average Loss value 0.75312847603\n",
      "weight distance : 1.05382455198e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 19500 : Average Loss value 0.751298769915\n",
      "weight distance : 1.05612667953e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 19600 : Average Loss value 0.753655450546\n",
      "weight distance : 1.02186270388e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 19700 : Average Loss value 0.753130653919\n",
      "weight distance : 1.05382531789e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 19800 : Average Loss value 0.751300318278\n",
      "weight distance : 1.05612926976e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 19900 : Average Loss value 0.753657842106\n",
      "weight distance : 1.02186466843e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 20000 : Average Loss value 0.753133092383\n",
      "weight distance : 1.05382608226e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 20100 : Average Loss value 0.751302127201\n",
      "weight distance : 1.05613185729e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 20200 : Average Loss value 0.753660494161\n",
      "weight distance : 1.02186663057e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 20300 : Average Loss value 0.753135790992\n",
      "weight distance : 1.0538268451e-06\n",
      "Testing 0/1 loss : 0.2223\n",
      "Iteration 20400 : Average Loss value 0.751304196251\n",
      "weight distance : 1.05613444212e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 20500 : Average Loss value 0.753663406279\n",
      "weight distance : 1.0218685903e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 20600 : Average Loss value 0.753138749313\n",
      "weight distance : 1.05382760641e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 20700 : Average Loss value 0.751306524996\n",
      "weight distance : 1.05613702425e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 20800 : Average Loss value 0.75366657803\n",
      "weight distance : 1.02187054761e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 20900 : Average Loss value 0.753141966916\n",
      "weight distance : 1.0538283662e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 21000 : Average Loss value 0.751309113007\n",
      "weight distance : 1.05613960367e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 21100 : Average Loss value 0.753670008983\n",
      "weight distance : 1.02187250251e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 21200 : Average Loss value 0.75314544337\n",
      "weight distance : 1.05382912447e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 21300 : Average Loss value 0.751311959853\n",
      "weight distance : 1.05614218041e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 21400 : Average Loss value 0.753673698709\n",
      "weight distance : 1.021874455e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 21500 : Average Loss value 0.753149178247\n",
      "weight distance : 1.05382988122e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 21600 : Average Loss value 0.751315065104\n",
      "weight distance : 1.05614475445e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 21700 : Average Loss value 0.753677646779\n",
      "weight distance : 1.02187640508e-06\n",
      "Testing 0/1 loss : 0.2222\n",
      "Iteration 21800 : Average Loss value 0.753153171119\n",
      "weight distance : 1.05383063645e-06\n",
      "Testing 0/1 loss : 0.2221\n",
      "Iteration 21900 : Average Loss value 0.751318428333\n",
      "weight distance : 1.0561473258e-06\n",
      "Testing 0/1 loss : 0.2221\n",
      "Iteration 22000 : Average Loss value 0.753681852765\n",
      "weight distance : 1.02187835277e-06\n",
      "Testing 0/1 loss : 0.2221\n",
      "Iteration 22100 : Average Loss value 0.753157421557\n",
      "weight distance : 1.05383139016e-06\n",
      "Testing 0/1 loss : 0.2221\n",
      "Iteration 22200 : Average Loss value 0.751322049113\n",
      "weight distance : 1.05614989446e-06\n",
      "Testing 0/1 loss : 0.2221\n",
      "Iteration 22300 : Average Loss value 0.75368631624\n",
      "weight distance : 1.02188029805e-06\n",
      "Testing 0/1 loss : 0.2221\n",
      "Iteration 22400 : Average Loss value 0.753161929136\n",
      "weight distance : 1.05383214237e-06\n",
      "Testing 0/1 loss : 0.2221\n",
      "Iteration 22500 : Average Loss value 0.751325927016\n",
      "weight distance : 1.05615246045e-06\n",
      "Testing 0/1 loss : 0.2221\n",
      "Iteration 22600 : Average Loss value 0.753691036778\n",
      "weight distance : 1.02188224092e-06\n",
      "Testing 0/1 loss : 0.2221\n",
      "Iteration 22700 : Average Loss value 0.75316669343\n",
      "weight distance : 1.05383289307e-06\n",
      "Testing 0/1 loss : 0.2221\n",
      "Iteration 22800 : Average Loss value 0.751330061616\n",
      "weight distance : 1.05615502374e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 22900 : Average Loss value 0.753696013954\n",
      "weight distance : 1.0218841814e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 23000 : Average Loss value 0.753171714014\n",
      "weight distance : 1.05383364227e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 23100 : Average Loss value 0.75133445249\n",
      "weight distance : 1.05615758436e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 23200 : Average Loss value 0.753701247343\n",
      "weight distance : 1.02188611948e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 23300 : Average Loss value 0.753176990463\n",
      "weight distance : 1.05383438996e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 23400 : Average Loss value 0.751339099212\n",
      "weight distance : 1.0561601423e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 23500 : Average Loss value 0.753706736521\n",
      "weight distance : 1.02188805517e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 23600 : Average Loss value 0.753182522355\n",
      "weight distance : 1.05383513616e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 23700 : Average Loss value 0.75134400136\n",
      "weight distance : 1.05616269757e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 23800 : Average Loss value 0.753712481066\n",
      "weight distance : 1.02188998846e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 23900 : Average Loss value 0.753188309266\n",
      "weight distance : 1.05383588086e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 24000 : Average Loss value 0.751349158511\n",
      "weight distance : 1.05616525017e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 24100 : Average Loss value 0.753718480555\n",
      "weight distance : 1.02189191935e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 24200 : Average Loss value 0.753194350776\n",
      "weight distance : 1.05383662407e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 24300 : Average Loss value 0.751354570243\n",
      "weight distance : 1.05616780011e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 24400 : Average Loss value 0.753724734567\n",
      "weight distance : 1.02189384786e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 24500 : Average Loss value 0.753200646464\n",
      "weight distance : 1.05383736578e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 24600 : Average Loss value 0.751360236136\n",
      "weight distance : 1.05617034737e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 24700 : Average Loss value 0.753731242682\n",
      "weight distance : 1.02189577399e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 24800 : Average Loss value 0.753207195909\n",
      "weight distance : 1.05383810602e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 24900 : Average Loss value 0.751366155769\n",
      "weight distance : 1.05617289198e-06\n",
      "Testing 0/1 loss : 0.222\n",
      "Iteration 25000 : Average Loss value 0.75373800448\n",
      "weight distance : 1.02189769772e-06\n",
      "Testing 0/1 loss : 0.222\n"
     ]
    }
   ],
   "source": [
    "model.initIteration();\n",
    "model.load_batch();\n",
    "model.computeProb();\n",
    "model.computeLoss();\n",
    "model.computeTestLoss()\n",
    "\n",
    "aveloss = np.mean(model.lossSet[max(0, model.iter-50) : model.iter+1]);\n",
    "print ('Iteration 0 : Loss value ' + str(model.loss))\n",
    "\n",
    "while (aveloss - model.prevLoss>0.5 or model.iter<25000):\n",
    "#while (model.prevLoss - model.loss > model.thresh or model.iter<10):\n",
    "    if model.iter==4000:\n",
    "        model.eta = 0.000001;\n",
    "    elif model.iter==15000:\n",
    "        model.eta = 0.0000001;\n",
    "    model.load_batch();\n",
    "    \n",
    "    model.prevLoss = aveloss;\n",
    "    model.iter = model.iter + 1;\n",
    "    temp = model.weights.copy();\n",
    "    \n",
    "    model.updateWeightsWhole()\n",
    "    model.computeProb();\n",
    "    \n",
    "    model.computeLoss()\n",
    "    model.computeTestLoss()\n",
    "    model.distWeight(temp, model.weights)\n",
    "    aveloss = np.mean(model.lossSet[max(0, model.iter-50) : model.iter+1]);\n",
    "    if np.floor(np.float(model.iter)/100) * 100 == model.iter:\n",
    "        print ('Iteration ' + str(model.iter) + ' : Average Loss value ' + str(aveloss))\n",
    "        print(\"weight distance : \" + str(model.weightDist));\n",
    "        print(\"Testing 0/1 loss : \" + str(model.testLoss01Set[model.iter]));\n",
    "        \n",
    "model.computeTrainLoss()\n",
    "model.plotLoss('training loss', 'testing loss', 'stochloss.png')\n",
    "model.plotLoss01('training loss', 'testing loss', 'stochloss01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116ecb190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "model.plotLoss01('training loss', 'testing loss', 'neuralLogisticLoss01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFdCAYAAABfMCThAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYCFJksrmfwFPzrlWmVSIPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114772110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "model.plotLoss('training loss', 'testing loss', 'neuralLogisticLoss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7291651201773266"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.testLossSet[model.iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
