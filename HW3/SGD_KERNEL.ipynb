{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "import mnist\n",
    "import scipy\n",
    "import scipy.sparse.linalg\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import identity\n",
    "import pdb\n",
    "from scipy.linalg import norm\n",
    "from scipy.spatial.distance import pdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "MNIST loaded\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = mnist.load_mnist(\"training\", None, './MNIST');\n",
    "testX, testY = mnist.load_mnist(\"testing\", None, './MNIST');\n",
    "\n",
    "print(str(trainX.shape))\n",
    "print(\"MNIST loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SGDRBF:\n",
    "    \n",
    "    \n",
    "    def __init__(self, eta, lamda, thresh, batchSize, kernelBW):\n",
    "        self.eta = eta;\n",
    "        self.lamda = lamda;\n",
    "        self.thresh = thresh;\n",
    "        self.regularized = True;\n",
    "        self.lossSet = np.zeros(100000);\n",
    "        self.testLossSet = np.zeros(100000);\n",
    "        self.testLossSetAve = np.zeros(100000);\n",
    "        self.loss01SetTr = np.zeros(100000);\n",
    "        self.loss01SetTe = np.zeros(100000);\n",
    "        self.loss01SetTeAve = np.zeros(100000);\n",
    "        self.trainLossSet = np.zeros(100000);\n",
    "        self.trainLossSetAve = np.zeros(100000);\n",
    "        self.loss01SetTrAve = np.zeros(100000);\n",
    "        self.batchSize = batchSize;\n",
    "        self.kernelBW = kernelBW;\n",
    "        self.stoch = True;\n",
    "        \n",
    "    def load_train(self, trainX, trainY):\n",
    "        self.trainY1 = trainY;\n",
    "        s = trainX.shape;\n",
    "        self.d = s[1] * s[2] + 1;\n",
    "        #self.d = s[1] + 1;\n",
    "        self.n = self.d-1;\n",
    "        self.N = s[0];\n",
    "        self.trainX = np.asmatrix(np.reshape(trainX, (self.N, self.d-1)));\n",
    "        #pdb.set_trace()\n",
    "        self.K = np.max(trainY) + 1\n",
    "        self.trainY = np.zeros((self.N, self.K),dtype=np.float)\n",
    "  \n",
    "        for i in range(0,self.K):\n",
    "            temp = np.reshape(np.array(trainY == i, dtype=float), (self.N, 1));\n",
    "            self.trainY[:,i] = np.asmatrix(np.reshape(temp, self.N))\n",
    "            \n",
    "        if self.stoch==False:\n",
    "            self.probTr = np.asmatrix(np.zeros((self.N, self.K),dtype=float))\n",
    "        else:\n",
    "            self.probTr = np.asmatrix(np.zeros((self.batchSize, self.K),dtype=float))\n",
    "        self.probTr1 = np.asmatrix(np.zeros((self.N, self.K),dtype=float))\n",
    "        print(\"Training data loaded...\")\n",
    "        \n",
    "    def load_test(self, testX, testY):\n",
    "        self.testY1 = testY;\n",
    "        #pdb.set_trace()\n",
    "        self.testN = testX.shape[0];\n",
    "        self.testX = np.asmatrix(np.reshape(testX, (self.testN, self.d-1)));\n",
    "        #pdb.set_trace()\n",
    "        self.testX = np.sin(self.testX * self.rand_kernel / self.kernelBW)\n",
    "        self.testX = np.insert(self.testX, 0, 1, axis=1);\n",
    "        \n",
    "        self.testY = np.zeros((self.testN, self.K),dtype=np.float)\n",
    "        self.probTe = np.asmatrix(np.zeros((self.testN, self.K),dtype=float))\n",
    "        \n",
    "        for i in range(0,self.K):\n",
    "            temp = np.reshape(np.array(testY == i, dtype=float), (self.testN, 1));\n",
    "            self.testY[:,i] = np.asmatrix(np.reshape(temp, self.testN))\n",
    "        print(\"Testing data loaded...\")\n",
    "            \n",
    "    def load_batch(self):\n",
    "        #print (str(self.batchFrom) + ' ' + str(self.batchTo))\n",
    "        self.Y1 = self.trainY1[self.perm[self.batchFrom:self.batchTo]]\n",
    "        self.X = np.sin(self.trainX[self.perm[self.batchFrom:self.batchTo], :] * self.rand_kernel / self.kernelBW)\n",
    "        self.X = np.insert(self.X, 0, 1, axis=1);\n",
    "        self.Y = self.trainY[self.perm[self.batchFrom:self.batchTo], :]\n",
    "        self.batchFrom += self.batchSize\n",
    "        self.batchTo += self.batchSize\n",
    "        self.batchFrom = (self.batchFrom - self.N) if self.batchFrom >= self.N else self.batchFrom\n",
    "        self.batchTo = (self.batchTo - self.N) if self.batchTo > self.N else self.batchTo\n",
    "        if self.batchFrom > self.batchTo or self.batchFrom==0:\n",
    "            self.newBatch = True;\n",
    "            self.batchFrom += self.batchSize\n",
    "            self.batchTo += self.batchSize\n",
    "            self.batchFrom = (self.batchFrom - self.N) if self.batchFrom >= self.N else self.batchFrom\n",
    "            self.batchTo = (self.batchTo - self.N) if self.batchTo > self.N else self.batchTo\n",
    "            self.iterAll = self.iterAll + 1;\n",
    "            self.perm = np.random.permutation(self.N)\n",
    "        else:\n",
    "            self.newBatch = False;\n",
    "        \n",
    "        \n",
    "    def initIteration(self):\n",
    "        self.weights = np.asmatrix(np.zeros((self.N + 1, self.K), dtype=float));\n",
    "        self.iter = 0;\n",
    "        self.iterAll = 0;\n",
    "        self.prevLoss = 999999;\n",
    "        self.weightDist = 99999;\n",
    "        self.batchFrom = 0;\n",
    "        self.batchTo = self.batchSize;\n",
    "        self.perm = range(0, self.N)\n",
    "        \n",
    "    \n",
    "    def computeTrainLoss(self):\n",
    "        trainLoss = 0;  trainLossAve = 0;\n",
    "        total = 0; batch = 5000; totalAve = 0;\n",
    "        for i in range(0,self.N/batch):\n",
    "            print(\"Evaluating training accuracy : Part \" + str(i+1) + \"....\")\n",
    "            Y1 = self.trainY1[i*batch : i*batch+1000]\n",
    "            X = np.sin(self.trainX[i*batch : i*batch+1000, :] * self.rand_kernel / self.kernelBW)\n",
    "            X = np.insert(X, 0, 1, axis=1);\n",
    "            Y = self.trainY[i*batch : i*batch+1000, :]\n",
    "            pred_y = X * self.linearWeight\n",
    "            pred_yAve = X * self.weightAve\n",
    "            predLabelTr = np.argmax(pred_y, axis=1);\n",
    "            predLabelTrAve = np.argmax(pred_yAve, axis=1);\n",
    "            trainLoss += np.sum(np.square(Y - pred_y));\n",
    "            trainLossAve += np.sum(np.square(Y - pred_yAve));\n",
    "            \n",
    "            temp = (np.array(predLabelTr) != np.reshape(Y1, (batch/5, 1))).astype(float)\n",
    "            total += np.sum(temp);\n",
    "            temp = (np.array(predLabelTrAve) != np.reshape(Y1, (batch/5, 1))).astype(float)\n",
    "            totalAve += np.sum(temp);\n",
    "            #pdb.set_trace()\n",
    "        self.loss01SetTr[self.iterA] = np.float(total) / self.N * 5; \n",
    "        self.loss01SetTrAve[self.iterA] = np.float(totalAve) / self.N * 5; \n",
    "        \n",
    "        self.trainLossSet[self.iterA] = trainLoss /  2 / self.N * 5 + self.lamda * norm(self.weightAve, 2);\n",
    "        self.trainLossSetAve[self.iterA] = trainLossAve /  2 / self.N * 5 + self.lamda * norm(self.weightAve, 2);\n",
    "    \n",
    "    def computeTestLoss(self):\n",
    "        self.predT_y = self.testX * self.linearWeight;\n",
    "        self.predT_yAve = self.testX * self.weightAve;\n",
    "        self.testLossSet[self.iterA] = np.sum(np.square(self.testY - self.predT_y)) / 2 / self.testN + self.lamda * norm(self.weightAve, 2);\n",
    "        self.testLossSetAve[self.iterA] = np.sum(np.square(self.testY - self.predT_yAve)) / 2 / self.testN + self.lamda * norm(self.weightAve, 2);\n",
    "        self.predLabelTe = np.argmax(self.predT_y, axis=1);\n",
    "        self.predLabelTeAve = np.argmax(self.predT_yAve, axis=1);\n",
    "        temp = (np.array(self.predLabelTe) != np.reshape(self.testY1, (self.testN, 1))).astype(float)\n",
    "        self.loss01SetTe[self.iterA] = np.sum(temp) / self.testN;\n",
    "        temp = (np.array(self.predLabelTeAve) != np.reshape(self.testY1, (self.testN, 1))).astype(float)\n",
    "        self.loss01SetTeAve[self.iterA] = np.sum(temp) / self.testN;\n",
    "        \n",
    "\n",
    "\n",
    "    def linearfit(self, flag_print_status, flag_eval, trainY):\n",
    "\n",
    "        # Initialization\n",
    "        self.linearWeight = np.asmatrix(np.zeros( (self.N+1, self.K) ));\n",
    "        self.iter = 0;\n",
    "        self.iterA = 0;\n",
    "        converge = False;\n",
    "        \n",
    "        self.load_batch();\n",
    "        \n",
    "        self.pred_y = self.X * self.linearWeight;\n",
    "        self.weightAve = np.zeros((self.N+1, 10));\n",
    "        \n",
    "        # Start Iterations\n",
    "        idx = 0;\n",
    "        while not converge:\n",
    "            if (self.iter>12000):\n",
    "                self.eta = 0.0001;\n",
    "            \n",
    "            #if (flag_print_status):\n",
    "            #            print (\"Processing Epoch \" + str(self.iter));\n",
    "            \n",
    "            if self.iter > 0:\n",
    "                self.load_batch();\n",
    "            self.iter += 1;\n",
    "            \n",
    "            #pdb.set_trace()\n",
    "            self.pred_y = self.X * self.linearWeight;\n",
    "            gradient = - self.X.transpose() * (np.asmatrix(self.Y) - self.pred_y) / self.batchSize;\n",
    "            gradient = gradient + 2 * self.lamda * self.linearWeight;\n",
    "            self.linearWeight = self.linearWeight - self.eta * gradient;\n",
    "            self.weightAve += self.linearWeight;\n",
    "            \n",
    "            self.trainLoss = np.sum(np.square(self.Y - self.pred_y)) / 2 / self.batchSize + self.lamda * norm(self.linearWeight, 2);\n",
    "            #print(\"\\tTraining loss : \" + str(self.trainLoss))\n",
    "            idx += 1;\n",
    "            \n",
    "            if self.newBatch or np.floor(np.float(self.batchFrom)/20000)==np.float(self.batchFrom)/20000:\n",
    "                self.weightAve = self.weightAve / idx;\n",
    "                model.computeTrainLoss()\n",
    "                model.computeTestLoss()\n",
    "                \n",
    "                idx = 0;\n",
    "                if (flag_print_status):\n",
    "                    print (\"Processing Iteration \" + str(self.iter));\n",
    "                    print(\"Training loss : \" + str(self.trainLossSet[self.iterA]))\n",
    "                    print(\"Testing loss : \" + str(self.testLossSet[self.iterA]))\n",
    "                    print(\"Training 0/1 loss : \" + str(self.loss01SetTr[self.iterA]))\n",
    "                    print(\"Testing 0/1 loss : \" + str(self.loss01SetTe[self.iterA]))\n",
    "                    print(\"Training 0/1 Ave loss : \" + str(self.loss01SetTrAve[self.iterA]))\n",
    "                    print(\"Testing 0/1 Ave loss : \" + str(self.loss01SetTeAve[self.iterA]))\n",
    "                    print(\"Training Ave loss : \" + str(self.trainLossSetAve[self.iterA]))\n",
    "                    print(\"Testing Ave loss : \" + str(self.testLossSetAve[self.iterA]))\n",
    "                self.iterA += 1;\n",
    "            converge = False if self.trainLoss > self.thresh or self.iter < 3 or self.iterAll<4 else True;\n",
    "            \n",
    "    \n",
    "    def generateWeight(self, sigma):\n",
    "        self.rand_kernel = np.asmatrix(np.random.normal(0, sigma, (self.n, self.N)));\n",
    "        print(\"Kernel generated\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded...\n",
      "Kernel generated\n",
      "Testing data loaded...\n",
      "Evaluating training accuracy : Part 1....\n",
      "Evaluating training accuracy : Part 2....\n",
      "Evaluating training accuracy : Part 3....\n",
      "Evaluating training accuracy : Part 4....\n",
      "Evaluating training accuracy : Part 5....\n",
      "Evaluating training accuracy : Part 6....\n",
      "Evaluating training accuracy : Part 7....\n",
      "Evaluating training accuracy : Part 8....\n",
      "Evaluating training accuracy : Part 9....\n",
      "Evaluating training accuracy : Part 10....\n",
      "Evaluating training accuracy : Part 11....\n",
      "Evaluating training accuracy : Part 12....\n",
      "Processing Iteration 2000\n",
      "Training loss : 0.0834023927178\n",
      "Testing loss : 0.0908049663721\n",
      "Training 0/1 loss : 0.0179166666667\n",
      "Testing 0/1 loss : 0.0284\n",
      "Training 0/1 Ave loss : 0.0186666666667\n",
      "Testing 0/1 Ave loss : 0.0269\n",
      "Training Ave loss : 0.0499073111103\n",
      "Testing Ave loss : 0.0563344971064\n",
      "Evaluating training accuracy : Part 1....\n",
      "Evaluating training accuracy : Part 2....\n",
      "Evaluating training accuracy : Part 3....\n",
      "Evaluating training accuracy : Part 4....\n",
      "Evaluating training accuracy : Part 5....\n",
      "Evaluating training accuracy : Part 6....\n",
      "Evaluating training accuracy : Part 7....\n",
      "Evaluating training accuracy : Part 8....\n",
      "Evaluating training accuracy : Part 9....\n",
      "Evaluating training accuracy : Part 10....\n",
      "Evaluating training accuracy : Part 11....\n",
      "Evaluating training accuracy : Part 12....\n",
      "Processing Iteration 4000\n",
      "Training loss : 0.0768949711084\n",
      "Testing loss : 0.0917532622897\n",
      "Training 0/1 loss : 0.00825\n",
      "Testing 0/1 loss : 0.0218\n",
      "Training 0/1 Ave loss : 0.008\n",
      "Testing 0/1 Ave loss : 0.0187\n",
      "Training Ave loss : 0.0397414689807\n",
      "Testing Ave loss : 0.0555957828983\n",
      "Evaluating training accuracy : Part 1....\n",
      "Evaluating training accuracy : Part 2....\n",
      "Evaluating training accuracy : Part 3....\n",
      "Evaluating training accuracy : Part 4....\n",
      "Evaluating training accuracy : Part 5....\n",
      "Evaluating training accuracy : Part 6....\n",
      "Evaluating training accuracy : Part 7....\n",
      "Evaluating training accuracy : Part 8....\n",
      "Evaluating training accuracy : Part 9....\n",
      "Evaluating training accuracy : Part 10....\n",
      "Evaluating training accuracy : Part 11....\n",
      "Evaluating training accuracy : Part 12....\n",
      "Processing Iteration 6000\n",
      "Training loss : 0.0746941358034\n",
      "Testing loss : 0.0957794807798\n",
      "Training 0/1 loss : 0.00391666666667\n",
      "Testing 0/1 loss : 0.023\n",
      "Training 0/1 Ave loss : 0.00116666666667\n",
      "Testing 0/1 Ave loss : 0.0181\n",
      "Training Ave loss : 0.0346813573356\n",
      "Testing Ave loss : 0.0595557783821\n",
      "Evaluating training accuracy : Part 1....\n",
      "Evaluating training accuracy : Part 2....\n",
      "Evaluating training accuracy : Part 3....\n",
      "Evaluating training accuracy : Part 4....\n",
      "Evaluating training accuracy : Part 5....\n",
      "Evaluating training accuracy : Part 6....\n",
      "Evaluating training accuracy : Part 7....\n",
      "Evaluating training accuracy : Part 8....\n",
      "Evaluating training accuracy : Part 9....\n",
      "Evaluating training accuracy : Part 10....\n",
      "Evaluating training accuracy : Part 11....\n",
      "Evaluating training accuracy : Part 12....\n",
      "Processing Iteration 7999\n",
      "Training loss : 0.0533336967764\n",
      "Testing loss : 0.0822488234949\n",
      "Training 0/1 loss : 0.0015\n",
      "Testing 0/1 loss : 0.0218\n",
      "Training 0/1 Ave loss : 0.00108333333333\n",
      "Testing 0/1 Ave loss : 0.0183\n",
      "Training Ave loss : 0.0309842533487\n",
      "Testing Ave loss : 0.060033410513\n",
      "Evaluating training accuracy : Part 1....\n",
      "Evaluating training accuracy : Part 2....\n",
      "Evaluating training accuracy : Part 3....\n",
      "Evaluating training accuracy : Part 4....\n",
      "Evaluating training accuracy : Part 5....\n",
      "Evaluating training accuracy : Part 6....\n",
      "Evaluating training accuracy : Part 7....\n",
      "Evaluating training accuracy : Part 8....\n",
      "Evaluating training accuracy : Part 9....\n",
      "Evaluating training accuracy : Part 10....\n",
      "Evaluating training accuracy : Part 11....\n",
      "Evaluating training accuracy : Part 12....\n",
      "Processing Iteration 9999\n",
      "Training loss : 0.0521348323453\n",
      "Testing loss : 0.0839414908087\n",
      "Training 0/1 loss : 0.00133333333333\n",
      "Testing 0/1 loss : 0.0212\n",
      "Training 0/1 Ave loss : 0.000833333333333\n",
      "Testing 0/1 Ave loss : 0.0177\n",
      "Training Ave loss : 0.0259917595578\n",
      "Testing Ave loss : 0.0592177424059\n",
      "Evaluating training accuracy : Part 1....\n",
      "Evaluating training accuracy : Part 2....\n",
      "Evaluating training accuracy : Part 3....\n",
      "Evaluating training accuracy : Part 4....\n",
      "Evaluating training accuracy : Part 5....\n",
      "Evaluating training accuracy : Part 6....\n",
      "Evaluating training accuracy : Part 7....\n",
      "Evaluating training accuracy : Part 8....\n",
      "Evaluating training accuracy : Part 9....\n",
      "Evaluating training accuracy : Part 10....\n",
      "Evaluating training accuracy : Part 11....\n",
      "Evaluating training accuracy : Part 12....\n",
      "Processing Iteration 11999\n",
      "Training loss : 0.0474479708121\n",
      "Testing loss : 0.0827080133251\n",
      "Training 0/1 loss : 0.000583333333333\n",
      "Testing 0/1 loss : 0.0208\n",
      "Training 0/1 Ave loss : 8.33333333333e-05\n",
      "Testing 0/1 Ave loss : 0.0179\n",
      "Training Ave loss : 0.0233205376561\n",
      "Testing Ave loss : 0.059719934247\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-e72c15619b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinearfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-6481bba759dd>\u001b[0m in \u001b[0;36mlinearfit\u001b[0;34m(self, flag_print_status, flag_eval, trainY)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-6481bba759dd>\u001b[0m in \u001b[0;36mload_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m#print (str(self.batchFrom) + ' ' + str(self.batchTo))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainY1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchFrom\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchTo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchFrom\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchTo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand_kernel\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernelBW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchFrom\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchTo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SGDRBF(0.0005, 0, 0.02, 10, 3.76);\n",
    "\n",
    "model.load_train(trainX, trainY);\n",
    "model.generateWeight(1.0);\n",
    "model.load_test(testX, testY);\n",
    "\n",
    "model.initIteration();\n",
    "\n",
    "model.linearfit(True, 2, trainY);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.0418\n"
     ]
    }
   ],
   "source": [
    "#model.trainX = np.insert(model.trainX, 0, 1, axis=1);\n",
    "#model.trainX = np.sin(model.trainX[:,0:784] * model.rand_kernel / model.kernelBW)\n",
    "#model.pred_y = model.trainX * model.linearWeight;\n",
    "#model.predLabelTr = np.argmax(model.pred_y, axis=1);\n",
    "#temp = (np.array(model.predLabelTr) != np.reshape(trainY, (model.N, 1))).astype(float)\n",
    "#model.loss01SetTr[model.iter] = np.float(np.sum(temp)) / np.float(model.N);\n",
    "#print(\"Train Loss : \" + str(model.loss01SetTr[model.iter]))\n",
    "\n",
    "#model.testX = np.sin(model.testX * model.rand_kernel / model.kernelBW)\n",
    "model.predT_y = model.testX * model.linearWeight;\n",
    "model.testLoss = np.sum(np.square(model.testY - model.predT_y)) / 2 / model.testN;\n",
    "model.predLabelTe = np.argmax(model.predT_y, axis=1);\n",
    "temp = (np.array(model.predLabelTe) != np.reshape(model.testY1, (model.testN, 1))).astype(float)\n",
    "model.loss01SetTe[model.iter] = np.float(np.sum(temp)) / np.float(model.testN);\n",
    "print(\"Test Loss : \" + str(model.loss01SetTe[model.iter]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.pyplot.clf()\n",
    "length = sum(model.loss01SetTeAve>0)\n",
    "red_star = matplotlib.pyplot.plot(range(1, length+1), model.trainLossSet[0:length], color=\"blue\", hold = True, linewidth=2.0)\n",
    "red_star = matplotlib.pyplot.plot(range(1, length+1), model.testLossSet[0:length], color=\"red\", hold = True, linewidth=2.0)\n",
    "red_star = matplotlib.pyplot.plot(range(1, length+1), model.trainLossSetAve[0:length], color=\"green\", hold = True, linewidth=2.0)\n",
    "red_star = matplotlib.pyplot.plot(range(1, length+1), model.testLossSetAve[0:length], color=\"purple\", hold = True, linewidth=2.0)\n",
    "matplotlib.pyplot.savefig('Q212.png', transparent = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0690833333333\n",
      "0.0844833333333\n"
     ]
    }
   ],
   "source": [
    "print(str(model.loss01SetTr[model.iter]))\n",
    "print(str(model.loss01SetTrAve[model.iter]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib.pyplot.clf()\n",
    "length = sum(model.loss01SetTeAve>0)\n",
    "red_star = matplotlib.pyplot.plot(range(1, length+1), model.loss01SetTr[0:length], color=\"blue\", hold = True, linewidth=2.0)\n",
    "red_star = matplotlib.pyplot.plot(range(1, length+1), model.loss01SetTe[0:length], color=\"red\", hold = True, linewidth=2.0)\n",
    "red_star = matplotlib.pyplot.plot(range(1, length+1), model.loss01SetTrAve[0:length], color=\"green\", hold = True, linewidth=2.0)\n",
    "red_star = matplotlib.pyplot.plot(range(1, length+1), model.loss01SetTeAve[0:length], color=\"purple\", hold = True, linewidth=2.0)\n",
    "matplotlib.pyplot.savefig('Q213.png', transparent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(range(0, sum(model.loss01SetTeAve>0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.testLossSet[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
