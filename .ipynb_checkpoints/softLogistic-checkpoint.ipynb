{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "import mnist\n",
    "import scipy\n",
    "import scipy.sparse.linalg\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import identity\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST loaded\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = mnist.load_mnist(\"training\", None, './MNIST');\n",
    "testX, testY = mnist.load_mnist(\"testing\", None, './MNIST');\n",
    "print(\"MNIST loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class logistic:\n",
    "    \n",
    "    \n",
    "    def __init__(self, eta, lamda, thresh):\n",
    "        self.eta = eta;\n",
    "        self.lamda = lamda;\n",
    "        self.thresh = thresh;\n",
    "        self.regularized = True;\n",
    "        self.lossSet = np.zeros(2000);\n",
    "        self.testLossSet = np.zeros(2000);\n",
    "        \n",
    "    def load_train(self, trainX, trainY):\n",
    "        s = trainX.shape;\n",
    "        self.d = s[1] * s[2] + 1;\n",
    "        self.n = self.d;\n",
    "        self.N = s[0];\n",
    "        self.X = np.reshape(trainX, (self.N, self.d-1));\n",
    "        self.X = np.asmatrix(np.insert(self.X, 0, 1, axis=1));\n",
    "        self.Y = np.zeros((self.N, 10),dtype=np.float)\n",
    "        self.probTr = np.asmatrix(np.zeros((self.N, 10),dtype=float))\n",
    "        \n",
    "        for i in range(0,10):\n",
    "            temp = np.reshape(np.array(trainY == i, dtype=float), (self.N, 1));\n",
    "            self.Y[:,i] = np.asmatrix(np.reshape(temp, self.N))\n",
    "        print(\"Training data loaded...\")\n",
    "        \n",
    "    def load_test(self, testX, testY):\n",
    "        self.testN = testX.shape[0];\n",
    "        self.testX = np.reshape(testX, (self.testN, self.d-1));\n",
    "        self.testX = np.insert(self.testX, 0, 1, axis=1);\n",
    "        self.testY = np.zeros((self.testN, 10),dtype=np.float)\n",
    "        self.probTe = np.asmatrix(np.zeros((self.testN, 10),dtype=float))\n",
    "        \n",
    "        for i in range(0,10):\n",
    "            temp = np.reshape(np.array(testY == i, dtype=float), (self.testN, 1));\n",
    "            self.testY[:,i] = np.asmatrix(np.reshape(temp, self.testN))\n",
    "        print(\"Testing data loaded...\")\n",
    "    \n",
    "    def initIteration(self):\n",
    "        self.weights = np.asmatrix(np.zeros((self.n,10), dtype=float));\n",
    "        self.iter = 0;\n",
    "        self.prevLoss = 999999;\n",
    "        self.weightDist = 99999;\n",
    "        \n",
    "        \n",
    "    \n",
    "    def computeProb(self):\n",
    "        \n",
    "        temp = self.X * self.weights;\n",
    "        temp[temp>100] = 100\n",
    "        \n",
    "        temp = np.exp(temp);\n",
    "        sumCol = np.sum(temp, axis=1)\n",
    "        \n",
    "        for i in range(0,10):\n",
    "            self.probTr[:,i] = np.divide(temp[:,i], sumCol) \n",
    "        self.sampleLoss = self.Y - self.probTr; \n",
    "        \n",
    "    def computeLoss(self):\n",
    "        self.loss = -np.sum(np.log(np.sum(np.multiply(self.Y, self.probTr), axis=1) + 0.0000001)) / self.N; \n",
    "        self.lossSet[self.iter]=self.loss\n",
    "    \n",
    "    def computeTestLoss(self):\n",
    "        temp = self.testX * self.weights;\n",
    "        temp[temp>100] = 100\n",
    "        \n",
    "        temp = np.exp(temp);\n",
    "        sumCol = np.sum(temp, axis=1)\n",
    "        \n",
    "        for i in range(0,10):\n",
    "            self.probTe[:,i] = np.divide(temp[:,i], sumCol) \n",
    "        \n",
    "        self.testLoss = -np.sum(np.log(np.sum(np.multiply(self.testY, self.probTe), axis=1) + 0.0000001)) / self.testN; \n",
    "        self.testLossSet[self.iter]=self.testLoss\n",
    "    \n",
    "    \n",
    "    def updateWeights(self, curIdx, batchSize):\n",
    "        #print('Weight ' + str(curIdx) + ' : ' + str(self.weights[curIdx]))\n",
    "        self.batchSize = batchSize;\n",
    "        idx_from = range(0, self.N, self.batchSize)\n",
    "        idx_to = idx_from[1:]\n",
    "        idx_to.extend([self.N])\n",
    "        \n",
    "        if self.regularized:\n",
    "            for x in range(0, len(idx_from)):\n",
    "                self.weights[curIdx] = self.weights[curIdx] + self.eta * np.sum(self.X[idx_from[x]:idx_to[x], curIdx] * self.sampleLoss[idx_from[x]:idx_to[x],0]);\n",
    "            self.weights[curIdx] = self.weights[curIdx] - self.eta * self.weights[curIdx] * self.lamda;\n",
    "            #self.weights[curIdx] = self.weights[curIdx] + self.eta * (np.sum(self.X[:, curIdx] * self.sampleLoss[:,0]) - self.weights[curIdx] * self.lamda);\n",
    "        else:\n",
    "            for x in range(0, len(idx_from)):\n",
    "                self.weights[curIdx] = self.weights[curIdx] + self.eta * np.sum(self.X[idx_from[x]:idx_to[x], curIdx] * self.sampleLoss[idx_from[x]:idx_to[x],0]);\n",
    "            #self.weights[curIdx] = self.weights[curIdx] + self.eta * np.sum(self.X[:, curIdx] * self.sampleLoss[:,0]);\n",
    "        #print('Weight ' + str(curIdx) + ' : ' + str(self.weights[curIdx]))\n",
    "        \n",
    "    def updateWeightsWhole(self):\n",
    "        deriv = self.Y - self.probTr;\n",
    "        self.weights = self.weights + self.eta * self.X.transpose() * deriv / self.N\n",
    "    \n",
    "    def distWeight(self, A, B):\n",
    "        self.weightDist = scipy.linalg.norm(A - B);\n",
    "        \n",
    "        print(\"weight distance : \" + str(self.weightDist));\n",
    "    \n",
    "    def updateWeightStochastic(self, curIdx, curSmp):\n",
    "        if self.regularized:\n",
    "            self.weights[curIdx] = self.weights[curIdx] + self.eta * (self.X[curSmp, curIdx] * self.sampleLoss[curSmp,0] - self.weights[curIdx] * self.lamda);\n",
    "        else:\n",
    "            self.weights[curIdx] = self.weights[curIdx] + self.eta * self.X[curSmp, curIdx] * self.sampleLoss[curSmp,0];\n",
    "    \n",
    "    def assess(self):\n",
    "        model.predLabel = np.argmax(model.probTr, axis=1);\n",
    "        temp = (model.predLabel != np.reshape(trainY, (model.N, 1))).astype(float)\n",
    "        model.loss01 = np.sum(temp) / model.N;\n",
    "        print(\"0 / 1 loss : \" + str(model.loss01));\n",
    "        \n",
    "        self.predLabel = np.argmax(self.probTr, axis=1);\n",
    "        self.pred = self.probTr > 0.5;\n",
    "        self.correct = np.sum(np.int16(self.pred == (self.Y==1)))\n",
    "        self.overallAccu = np.float(self.correct) / np.float(self.N);\n",
    "        self.loss01 = sum(abs(self.pred - self.Y)) / self.N\n",
    "        print('Accuracy : ' + str(self.overallAccu))\n",
    "        print('0 / 1 loss : ' + str(self.loss01))\n",
    "    \n",
    "    def genGaussFeatWeight(self, n):\n",
    "        self.n = n;\n",
    "        self.weight = np.random.randn(self.d, self.n);\n",
    "    \n",
    "    def randGaussFeatConv(self):\n",
    "        self.X = np.dot(self.X, self.weight);\n",
    "        \n",
    "    def plotLoss(self, labelTrain, labelTest, name):\n",
    "        %matplotlib inline \n",
    "        matplotlib.pyplot.plot(np.arange(0, model.iter), model.testLossSet[0:model.iter], hold = True, color=\"blue\", linewidth=2.0)\n",
    "        matplotlib.pyplot.plot(np.arange(0, model.iter), model.lossSet[0:model.iter], hold = True, color=\"red\", linewidth=2.0)\n",
    "        matplotlib.pyplot.savefig(name, transparent = True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded...\n",
      "Testing data loaded...\n"
     ]
    }
   ],
   "source": [
    "model = logistic(0.1, 0.01, 0.001);\n",
    "model.load_train(trainX, trainY);\n",
    "model.load_test(testX, testY);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : Loss value 2.30258409299\n",
      "weight distance : 0.104093194111\n",
      "Iteration 1 : Loss value 2.19694200854\n",
      "weight distance : 0.0991989038061\n",
      "Iteration 2 : Loss value 2.10055075046\n",
      "weight distance : 0.0952525924058\n",
      "Iteration 3 : Loss value 2.01154110478\n",
      "weight distance : 0.0916775576616\n",
      "Iteration 4 : Loss value 1.9290567765\n",
      "weight distance : 0.0882795415428\n",
      "Iteration 5 : Loss value 1.85257342268\n",
      "weight distance : 0.0850006145579\n",
      "Iteration 6 : Loss value 1.78167130132\n",
      "weight distance : 0.0818299518774\n",
      "Iteration 7 : Loss value 1.71596298254\n",
      "weight distance : 0.0787711075162\n",
      "Iteration 8 : Loss value 1.65507427545\n",
      "weight distance : 0.0758303883867\n",
      "Iteration 9 : Loss value 1.59864229782\n",
      "weight distance : 0.0730130358433\n",
      "Iteration 10 : Loss value 1.5463181352\n",
      "weight distance : 0.0703222114679\n",
      "Iteration 11 : Loss value 1.49776996434\n",
      "weight distance : 0.0677589466586\n",
      "Iteration 12 : Loss value 1.45268545187\n",
      "weight distance : 0.0653224016461\n",
      "Iteration 13 : Loss value 1.41077321333\n",
      "weight distance : 0.0630102069003\n",
      "Iteration 14 : Loss value 1.37176341886\n",
      "weight distance : 0.0608188067449\n",
      "Iteration 15 : Loss value 1.3354077103\n",
      "weight distance : 0.0587437745214\n",
      "Iteration 16 : Loss value 1.3014786015\n",
      "weight distance : 0.0567800867952\n",
      "Iteration 17 : Loss value 1.26976851648\n",
      "weight distance : 0.0549223524871\n",
      "Iteration 18 : Loss value 1.24008859488\n",
      "weight distance : 0.0531649976581\n",
      "Iteration 19 : Loss value 1.21226736815\n",
      "weight distance : 0.0515024096096\n",
      "Iteration 20 : Loss value 1.18614938468\n",
      "weight distance : 0.0499290455056\n",
      "Iteration 21 : Loss value 1.16159384075\n",
      "weight distance : 0.0484395112539\n",
      "Iteration 22 : Loss value 1.13847325637\n",
      "weight distance : 0.0470286162379\n",
      "Iteration 23 : Loss value 1.11667222114\n",
      "weight distance : 0.0456914089619\n",
      "Iteration 24 : Loss value 1.09608622467\n",
      "weight distance : 0.0444231979679\n",
      "Iteration 25 : Loss value 1.07662057863\n",
      "weight distance : 0.0432195616425\n",
      "Iteration 26 : Loss value 1.05818943188\n",
      "weight distance : 0.0420763498415\n",
      "Iteration 27 : Loss value 1.04071487679\n",
      "weight distance : 0.0409896796452\n",
      "Iteration 28 : Loss value 1.02412614229\n",
      "weight distance : 0.0399559270425\n",
      "Iteration 29 : Loss value 1.00835886817\n",
      "weight distance : 0.0389717159186\n",
      "Iteration 30 : Loss value 0.993354454115\n",
      "weight distance : 0.0380339053803\n",
      "Iteration 31 : Loss value 0.979059477103\n",
      "weight distance : 0.037139576186\n",
      "Iteration 32 : Loss value 0.965425170592\n",
      "weight distance : 0.0362860168387\n",
      "Iteration 33 : Loss value 0.952406959408\n",
      "weight distance : 0.0354707097402\n",
      "Iteration 34 : Loss value 0.939964044539\n",
      "weight distance : 0.0346913176831\n",
      "Iteration 35 : Loss value 0.928059032511\n",
      "weight distance : 0.0339456708652\n",
      "Iteration 36 : Loss value 0.916657604514\n",
      "weight distance : 0.0332317545446\n",
      "Iteration 37 : Loss value 0.905728220892\n",
      "weight distance : 0.0325476973992\n",
      "Iteration 38 : Loss value 0.895241857058\n",
      "weight distance : 0.0318917606253\n",
      "Iteration 39 : Loss value 0.885171767333\n",
      "weight distance : 0.031262327776\n",
      "Iteration 40 : Loss value 0.875493273559\n",
      "weight distance : 0.0306578953282\n",
      "Iteration 41 : Loss value 0.866183575727\n",
      "weight distance : 0.030077063953\n",
      "Iteration 42 : Loss value 0.857221582142\n",
      "weight distance : 0.0295185304543\n",
      "Iteration 43 : Loss value 0.848587756948\n",
      "weight distance : 0.0289810803404\n",
      "Iteration 44 : Loss value 0.840263983083\n",
      "weight distance : 0.0284635809862\n",
      "Iteration 45 : Loss value 0.832233438963\n",
      "weight distance : 0.0279649753465\n",
      "Iteration 46 : Loss value 0.824480487384\n",
      "weight distance : 0.0274842761804\n",
      "Iteration 47 : Loss value 0.816990575311\n",
      "weight distance : 0.0270205607467\n",
      "Iteration 48 : Loss value 0.809750143381\n",
      "weight distance : 0.0265729659339\n",
      "Iteration 49 : Loss value 0.80274654407\n",
      "weight distance : 0.0261406837897\n",
      "Iteration 50 : Loss value 0.795967967617\n",
      "weight distance : 0.0257229574162\n",
      "Iteration 51 : Loss value 0.789403374874\n",
      "weight distance : 0.0253190772017\n",
      "Iteration 52 : Loss value 0.783042436369\n",
      "weight distance : 0.0249283773593\n",
      "Iteration 53 : Loss value 0.776875476938\n",
      "weight distance : 0.0245502327467\n",
      "Iteration 54 : Loss value 0.770893425359\n",
      "weight distance : 0.0241840559432\n",
      "Iteration 55 : Loss value 0.765087768476\n",
      "weight distance : 0.0238292945622\n",
      "Iteration 56 : Loss value 0.759450509369\n",
      "weight distance : 0.0234854287778\n",
      "Iteration 57 : Loss value 0.753974129171\n",
      "weight distance : 0.0231519690489\n",
      "Iteration 58 : Loss value 0.748651552166\n",
      "weight distance : 0.0228284540219\n",
      "Iteration 59 : Loss value 0.743476113861\n",
      "weight distance : 0.0225144485986\n",
      "Iteration 60 : Loss value 0.738441531734\n",
      "weight distance : 0.0222095421535\n",
      "Iteration 61 : Loss value 0.733541878418\n",
      "weight distance : 0.0219133468897\n",
      "Iteration 62 : Loss value 0.728771557079\n",
      "weight distance : 0.0216254963194\n",
      "Iteration 63 : Loss value 0.7241252788\n",
      "weight distance : 0.0213456438608\n",
      "Iteration 64 : Loss value 0.719598041761\n",
      "weight distance : 0.0210734615402\n",
      "Iteration 65 : Loss value 0.715185112086\n",
      "weight distance : 0.0208086387907\n",
      "Iteration 66 : Loss value 0.710882006181\n",
      "weight distance : 0.0205508813409\n",
      "Iteration 67 : Loss value 0.706684474432\n",
      "weight distance : 0.0202999101836\n",
      "Iteration 68 : Loss value 0.702588486164\n",
      "weight distance : 0.020055460621\n",
      "Iteration 69 : Loss value 0.698590215729\n",
      "weight distance : 0.0198172813775\n",
      "Iteration 70 : Loss value 0.694686029632\n",
      "weight distance : 0.019585133777\n",
      "Iteration 71 : Loss value 0.690872474614\n",
      "weight distance : 0.0193587909773\n",
      "Iteration 72 : Loss value 0.687146266605\n",
      "weight distance : 0.0191380372594\n",
      "Iteration 73 : Loss value 0.68350428047\n",
      "weight distance : 0.0189226673656\n",
      "Iteration 74 : Loss value 0.679943540499\n",
      "weight distance : 0.0187124858829\n",
      "Iteration 75 : Loss value 0.676461211556\n",
      "weight distance : 0.0185073066694\n",
      "Iteration 76 : Loss value 0.673054590855\n",
      "weight distance : 0.0183069523188\n",
      "Iteration 77 : Loss value 0.669721100305\n",
      "weight distance : 0.0181112536612\n",
      "Iteration 78 : Loss value 0.666458279375\n",
      "weight distance : 0.0179200492969\n",
      "Iteration 79 : Loss value 0.663263778444\n",
      "weight distance : 0.017733185161\n",
      "Iteration 80 : Loss value 0.660135352598\n",
      "weight distance : 0.0175505141166\n",
      "Iteration 81 : Loss value 0.65707085583\n",
      "weight distance : 0.0173718955736\n",
      "Iteration 82 : Loss value 0.654068235636\n",
      "weight distance : 0.0171971951331\n",
      "Iteration 83 : Loss value 0.651125527942\n",
      "weight distance : 0.0170262842535\n",
      "Iteration 84 : Loss value 0.648240852377\n",
      "weight distance : 0.0168590399378\n",
      "Iteration 85 : Loss value 0.645412407826\n",
      "weight distance : 0.0166953444407\n",
      "Iteration 86 : Loss value 0.642638468283\n",
      "weight distance : 0.0165350849936\n",
      "Iteration 87 : Loss value 0.639917378944\n",
      "weight distance : 0.0163781535464\n",
      "Iteration 88 : Loss value 0.637247552557\n",
      "weight distance : 0.0162244465254\n",
      "Iteration 89 : Loss value 0.634627465982\n",
      "weight distance : 0.0160738646052\n",
      "Iteration 90 : Loss value 0.632055656966\n",
      "weight distance : 0.0159263124947\n",
      "Iteration 91 : Loss value 0.629530721112\n",
      "weight distance : 0.0157816987355\n",
      "Iteration 92 : Loss value 0.627051309019\n",
      "weight distance : 0.0156399355124\n",
      "Iteration 93 : Loss value 0.624616123603\n",
      "weight distance : 0.0155009384744\n",
      "Iteration 94 : Loss value 0.622223917561\n",
      "weight distance : 0.0153646265667\n",
      "Iteration 95 : Loss value 0.619873490991\n",
      "weight distance : 0.0152309218721\n",
      "Iteration 96 : Loss value 0.617563689141\n",
      "weight distance : 0.0150997494608\n",
      "Iteration 97 : Loss value 0.615293400289\n",
      "weight distance : 0.0149710372496\n",
      "Iteration 98 : Loss value 0.613061553741\n",
      "weight distance : 0.0148447158684\n",
      "Iteration 99 : Loss value 0.610867117939\n",
      "weight distance : 0.0147207185337\n",
      "Iteration 100 : Loss value 0.608709098675\n",
      "weight distance : 0.0145989809303\n",
      "Iteration 101 : Loss value 0.606586537397\n",
      "weight distance : 0.0144794410979\n",
      "Iteration 102 : Loss value 0.604498509617\n",
      "weight distance : 0.0143620393251\n",
      "Iteration 103 : Loss value 0.602444123393\n",
      "weight distance : 0.0142467180481\n",
      "Iteration 104 : Loss value 0.600422517896\n",
      "weight distance : 0.0141334217554\n",
      "Iteration 105 : Loss value 0.598432862057\n",
      "weight distance : 0.0140220968973\n",
      "Iteration 106 : Loss value 0.596474353275\n",
      "weight distance : 0.0139126918001\n",
      "Iteration 107 : Loss value 0.5945462162\n",
      "weight distance : 0.0138051565846\n",
      "Iteration 108 : Loss value 0.592647701573\n",
      "weight distance : 0.0136994430894\n",
      "Iteration 109 : Loss value 0.590778085128\n",
      "weight distance : 0.013595504797\n",
      "Iteration 110 : Loss value 0.588936666551\n",
      "weight distance : 0.0134932967649\n",
      "Iteration 111 : Loss value 0.587122768483\n",
      "weight distance : 0.0133927755592\n",
      "Iteration 112 : Loss value 0.585335735581\n",
      "weight distance : 0.013293899192\n",
      "Iteration 113 : Loss value 0.583574933623\n",
      "weight distance : 0.0131966270616\n",
      "Iteration 114 : Loss value 0.581839748651\n",
      "weight distance : 0.0131009198958\n",
      "Iteration 115 : Loss value 0.580129586164\n",
      "weight distance : 0.0130067396981\n",
      "Iteration 116 : Loss value 0.578443870343\n",
      "weight distance : 0.012914049696\n",
      "Iteration 117 : Loss value 0.576782043316\n",
      "weight distance : 0.0128228142925\n",
      "Iteration 118 : Loss value 0.575143564457\n",
      "weight distance : 0.012732999019\n",
      "Iteration 119 : Loss value 0.573527909717\n",
      "weight distance : 0.0126445704912\n",
      "Iteration 120 : Loss value 0.571934570988\n",
      "weight distance : 0.0125574963668\n",
      "Iteration 121 : Loss value 0.570363055494\n",
      "weight distance : 0.012471745305\n",
      "Iteration 122 : Loss value 0.56881288521\n",
      "weight distance : 0.0123872869281\n",
      "Iteration 123 : Loss value 0.56728359631\n",
      "weight distance : 0.0123040917845\n",
      "Iteration 124 : Loss value 0.565774738637\n",
      "weight distance : 0.0122221313142\n",
      "Iteration 125 : Loss value 0.564285875197\n",
      "weight distance : 0.0121413778146\n",
      "Iteration 126 : Loss value 0.562816581678\n",
      "weight distance : 0.012061804409\n",
      "Iteration 127 : Loss value 0.561366445985\n",
      "weight distance : 0.0119833850159\n",
      "Iteration 128 : Loss value 0.559935067801\n",
      "weight distance : 0.0119060943195\n",
      "Iteration 129 : Loss value 0.558522058166\n",
      "weight distance : 0.0118299077424\n",
      "Iteration 130 : Loss value 0.557127039068\n",
      "weight distance : 0.011754801418\n",
      "Iteration 131 : Loss value 0.555749643058\n",
      "weight distance : 0.0116807521656\n",
      "Iteration 132 : Loss value 0.554389512883\n",
      "weight distance : 0.0116077374658\n",
      "Iteration 133 : Loss value 0.553046301124\n",
      "weight distance : 0.0115357354368\n",
      "Iteration 134 : Loss value 0.551719669862\n",
      "weight distance : 0.011464724812\n",
      "Iteration 135 : Loss value 0.550409290349\n",
      "weight distance : 0.0113946849188\n",
      "Iteration 136 : Loss value 0.549114842695\n",
      "weight distance : 0.0113255956576\n",
      "Iteration 137 : Loss value 0.54783601557\n",
      "weight distance : 0.0112574374823\n",
      "Iteration 138 : Loss value 0.546572505913\n",
      "weight distance : 0.0111901913814\n",
      "Iteration 139 : Loss value 0.54532401866\n",
      "weight distance : 0.0111238388593\n",
      "Iteration 140 : Loss value 0.544090266477\n",
      "weight distance : 0.0110583619197\n",
      "Iteration 141 : Loss value 0.542870969505\n",
      "weight distance : 0.010993743048\n",
      "Iteration 142 : Loss value 0.541665855114\n",
      "weight distance : 0.0109299651961\n",
      "Iteration 143 : Loss value 0.540474657673\n",
      "weight distance : 0.0108670117662\n",
      "Iteration 144 : Loss value 0.53929711832\n",
      "weight distance : 0.0108048665963\n",
      "Iteration 145 : Loss value 0.538132984746\n",
      "weight distance : 0.0107435139463\n",
      "Iteration 146 : Loss value 0.536982010988\n",
      "weight distance : 0.0106829384836\n",
      "Iteration 147 : Loss value 0.535843957228\n",
      "weight distance : 0.0106231252706\n",
      "Iteration 148 : Loss value 0.534718589595\n",
      "weight distance : 0.0105640597516\n",
      "Iteration 149 : Loss value 0.533605679987\n",
      "weight distance : 0.0105057277412\n",
      "Iteration 150 : Loss value 0.532505005889\n",
      "weight distance : 0.0104481154117\n",
      "Iteration 151 : Loss value 0.531416350198\n",
      "weight distance : 0.0103912092829\n",
      "Iteration 152 : Loss value 0.53033950106\n",
      "weight distance : 0.0103349962105\n",
      "Iteration 153 : Loss value 0.529274251713\n",
      "weight distance : 0.0102794633758\n",
      "Iteration 154 : Loss value 0.528220400325\n",
      "weight distance : 0.010224598276\n",
      "Iteration 155 : Loss value 0.527177749857\n",
      "weight distance : 0.0101703887142\n",
      "Iteration 156 : Loss value 0.526146107908\n",
      "weight distance : 0.0101168227901\n",
      "Iteration 157 : Loss value 0.525125286588\n",
      "weight distance : 0.0100638888912\n",
      "Iteration 158 : Loss value 0.524115102378\n",
      "weight distance : 0.010011575684\n",
      "Iteration 159 : Loss value 0.523115376005\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFkCAYAAACjCwibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMfn/S2ohLa+njt8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1174e4510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.initIteration();\n",
    "model.computeProb();\n",
    "model.computeLoss();\n",
    "model.computeTestLoss()\n",
    "print ('Iteration 0 : Loss value ' + str(model.loss))\n",
    "\n",
    "#while (model.weightDist > 0.01 or model.iter<3):\n",
    "while (model.prevLoss - model.loss > model.thresh or model.iter<10):\n",
    "    model.prevLoss = model.loss;\n",
    "    model.iter = model.iter + 1;\n",
    "    temp = model.weights.copy();\n",
    "    \n",
    "    model.updateWeightsWhole()\n",
    "    model.computeProb();\n",
    "    \n",
    "        \n",
    "    model.computeLoss()\n",
    "    model.computeTestLoss()\n",
    "    model.distWeight(temp, model.weights)\n",
    "    print ('Iteration ' + str(model.iter) + ' : Loss value ' + str(model.loss))\n",
    "model.plotLoss('training loss', 'testing loss', 'softloss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 1 loss : 0.128116666667\n",
      "Training log loss : 0.523115376005\n",
      "0 / 1 loss : 0.1191\n",
      "Testing log loss : 0.499376937195\n"
     ]
    }
   ],
   "source": [
    "model.predLabel = np.argmax(model.probTr, axis=1);\n",
    "temp = (model.predLabel != np.reshape(trainY, (model.N, 1))).astype(float)\n",
    "model.loss01 = np.sum(temp) / model.N;\n",
    "print(\"0 / 1 loss : \" + str(model.loss01));\n",
    "print(\"Training log loss : \" + str(model.loss))\n",
    "        \n",
    "model.predLabel = np.argmax(model.probTe, axis=1);\n",
    "temp = (model.predLabel != np.reshape(testY, (model.testN, 1))).astype(float)\n",
    "model.loss01 = np.sum(temp) / model.testN;\n",
    "print(\"0 / 1 loss : \" + str(model.loss01));\n",
    "print(\"Testing log loss : \" + str(model.testLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
